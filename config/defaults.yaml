defaults:
  - data: classification
  - training: on_gpu
  - model: bert
  - learner: oml
  - hydra/job_logging: colorlog
  - hydra/hydra_logging: colorlog

learner:
  inner_lr: 1e-3
  meta_lr: 1e-5
  model_name: bert-base-uncased
  samples_per_task: # default None, which uses all data of each task
  evaluation_support_set: false # for meta learning methods, whether to use a support set when evaluating
  write_prob: 1.0 # Write probability for buffer memory
  replay_rate: 0.01 # replay rate from memory
  replay_every: 9600 # Number of data points between replay
  updates: 5 # number of inner loop updates
  clip_grad_norm: 25 # gradient norm clipping (for replay learner)

data:
  alternating_tasks: # Which tasks to alternate between. If none, uses all tasks
  alternating_n_samples_per_switch: 80 # How many samples to use at each switch.
  alternating_relative_frequencies: # How much each task is seen compared to others.

testing:
  few_shot_batch_size: 1 # for few shot evaluation
  average_accuracy: true # whether to compute average accuracy on all datasets
  few_shot: true # whether to perform few shot evaluation
  eval_dataset: amazon # which dataset to perform few shot evaluation on
  n_samples: 1024 # how many samples to train on during few shot evaluation
  few_shot_validation_size: 100 # sample a subset so validation doesn't take too long during few shot evaluation
  average_validation_size: # sample a subset so validation doesn't take too long during computation of average accuracy. use for debugging purposes
  eval_split: val # either "val" or "test", decides which split to use when evaluating

seed: 42 # random seed
inference: false
version: 0
task_order: # if not specified, use task order defined in text_classification_dataset.py
checkpoint_while_training: true # whether to checkpoint while training
save_checkpoint: false # whether to save checkpoint after training
save_optimizer_state: false # whether to save the optimizer state when checkpointing
delete_previous_checkpoint: true # whether to overwrite the previous checkpoint when saving a checkpoint
save_freq: 60 # in minutes, specifies how often a model is saved
wandb: true # whether to use weights and biases for logging
# set to debug mode.
# some extra logging is done, and a small dataset is used
debug_logging: true # log debug things
debug_data: false # use a small dataset
name: # experiment name, not specified by default, making it None

hydra:
  run:
    dir: experiments/${hydra.job.override_dirname}_${now:%Y-%m-%d_%H-%M-%S}_${name}/seed=${seed}
  sweep:
    dir: experiments/sweep-${name}_${now:%Y-%m-%d_%H-%M-%S}
    subdir: ${hydra.job.override_dirname}_${hydra.job.num}
  job:
    config:
      override_dirname:
        exclude_keys:
          - name
          - wandb
          - checkpoint_while_training
          - save_freq
          - seed